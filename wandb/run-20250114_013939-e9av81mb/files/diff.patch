diff --git a/environments.py b/environments.py
index 4808245..2638f3e 100644
--- a/environments.py
+++ b/environments.py
@@ -131,8 +131,8 @@ class EnvLoadRL(gym.Env):
         current             = self.current
         next_current        = np.clip(self.ad * current + self.bd * action_input, -self.i_max, self.i_max)
 
-        next_current_norm   = next_current/self.i_max
-        current_norm        = current/self.i_max
+        next_current_norm   =   next_current / self.i_max
+        current_norm        =        current / self.i_max
         reference_norm      = self.reference / self.i_max
 
         terminated = False
@@ -150,10 +150,9 @@ class EnvLoadRL(gym.Env):
     def reset(self, *, seed = None, options = None):
         super().reset(seed=seed)
 
-        low, high = [-0.9, 0.9]
-        current_norm    = np.round(self.np_random.uniform(low=low, high=high),6)
-        low, high = [-0.9, 0.9]
-        reference_norm  = np.round(self.np_random.uniform(low=low, high=high),6)
+        low, high = [-0.8, 0.8]
+        current_norm    = np.round(self.np_random.uniform(low=low, high=high),5)
+        reference_norm  = np.round(self.np_random.uniform(low=low, high=high),5)
         self.current    = self.i_max * current_norm
         self.reference  = self.i_max * reference_norm
 
diff --git a/train.py b/train.py
index 92c15bd..0922d90 100644
--- a/train.py
+++ b/train.py
@@ -2,6 +2,9 @@ import gymnasium as gym
 import numpy as np
 import matplotlib.pyplot as plt
 
+import wandb
+from wandb.integration.sb3 import WandbCallback
+
 from environments import EnvLoadRL, EnvLoadRLConst
 
 from stable_baselines3 import DDPG
@@ -19,11 +22,25 @@ if env_const:
     env = EnvLoadRLConst(sys_params=sys_params_dict)
 else:
     max_episode_steps = 500
-    max_episodes = 10000
+    max_episodes = 1000
     env = EnvLoadRL(sys_params=sys_params_dict)
 
 env = gym.wrappers.TimeLimit(env, max_episode_steps)
 
+# Wandb
+config = {
+    "policy_type": "MlpPolicy",
+    "total_timesteps": max_episodes*max_episode_steps,
+    "env_name": "RL Constant Reference" if env_const else "RL Variable Reference",
+}
+run = wandb.init(
+    project="sb3",
+    config=config,
+    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics
+    monitor_gym=True,  # auto-upload the videos of agents playing the game
+    save_code=True,  # optional
+)
+
 # The noise objects for DDPG
 n_actions = env.action_space.shape[-1]
 action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))
@@ -33,7 +50,7 @@ vec_env = model.get_env()
 
 train, test = (True, True)
 if train:
-    model.learn(total_timesteps=max_episodes*max_episode_steps, log_interval=10, progress_bar=True)
+    model.learn(total_timesteps=config["total_timesteps"], log_interval=10, progress_bar=True, callback=WandbCallback())
     model.save("ddpg_EnvLoadRL")
 if test:
     model = DDPG.load("ddpg_EnvLoadRL")
