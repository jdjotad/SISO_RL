diff --git a/environments.py b/environments.py
index 4808245..c32f72f 100644
--- a/environments.py
+++ b/environments.py
@@ -131,14 +131,17 @@ class EnvLoadRL(gym.Env):
         current             = self.current
         next_current        = np.clip(self.ad * current + self.bd * action_input, -self.i_max, self.i_max)
 
-        next_current_norm   = next_current/self.i_max
-        current_norm        = current/self.i_max
+        next_current_norm   =   next_current / self.i_max
+        current_norm        =        current / self.i_max
         reference_norm      = self.reference / self.i_max
 
         terminated = False
 
-        reward = -np.power(current_norm - reference_norm, 2)
-
+        tracking_error = np.power(current_norm - reference_norm, 2)
+        if tracking_error <= 1e-6:
+            reward = 1
+        else:
+            reward = -tracking_error
         # Observation: [current, reference]
         obs = np.array([next_current_norm, reference_norm], dtype=np.float32)
 
@@ -150,10 +153,9 @@ class EnvLoadRL(gym.Env):
     def reset(self, *, seed = None, options = None):
         super().reset(seed=seed)
 
-        low, high = [-0.9, 0.9]
-        current_norm    = np.round(self.np_random.uniform(low=low, high=high),6)
-        low, high = [-0.9, 0.9]
-        reference_norm  = np.round(self.np_random.uniform(low=low, high=high),6)
+        low, high = [-0.8, 0.8]
+        current_norm    = np.round(self.np_random.uniform(low=low, high=high),5)
+        reference_norm  = np.round(self.np_random.uniform(low=low, high=high),5)
         self.current    = self.i_max * current_norm
         self.reference  = self.i_max * reference_norm
 
diff --git a/train.py b/train.py
index 92c15bd..8585b0f 100644
--- a/train.py
+++ b/train.py
@@ -2,6 +2,9 @@ import gymnasium as gym
 import numpy as np
 import matplotlib.pyplot as plt
 
+import wandb
+from wandb.integration.sb3 import WandbCallback
+
 from environments import EnvLoadRL, EnvLoadRLConst
 
 from stable_baselines3 import DDPG
@@ -19,10 +22,24 @@ if env_const:
     env = EnvLoadRLConst(sys_params=sys_params_dict)
 else:
     max_episode_steps = 500
-    max_episodes = 10000
+    max_episodes = 1000
     env = EnvLoadRL(sys_params=sys_params_dict)
 
 env = gym.wrappers.TimeLimit(env, max_episode_steps)
+env = gym.wrappers.RecordEpisodeStatistics(env)
+
+# Wandb
+config = {
+    "policy_type": "MlpPolicy",
+    "total_timesteps": max_episodes*max_episode_steps,
+    "env_name": "RL Constant Reference" if env_const else "RL Variable Reference",
+}
+run = wandb.init(
+    project="sb3",
+    config=config,
+    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics
+    save_code=True,  # optional
+)
 
 # The noise objects for DDPG
 n_actions = env.action_space.shape[-1]
@@ -33,7 +50,7 @@ vec_env = model.get_env()
 
 train, test = (True, True)
 if train:
-    model.learn(total_timesteps=max_episodes*max_episode_steps, log_interval=10, progress_bar=True)
+    model.learn(total_timesteps=config["total_timesteps"], log_interval=10, progress_bar=True, verbose=1, tensorboard_log=f"runs/ddpg")
     model.save("ddpg_EnvLoadRL")
 if test:
     model = DDPG.load("ddpg_EnvLoadRL")
